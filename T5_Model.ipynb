{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2496734d-9fd1-4b00-a90f-a7e47bca9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 KB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.9/287.9 KB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "     -------------------------------------- 417.5/417.5 KB 1.1 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.7.24-cp39-cp39-win_amd64.whl (269 kB)\n",
      "     -------------------------------------- 269.7/269.7 KB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.5 regex-2024.7.24 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.43.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3472a868-d9de-4e3c-aa33-848b80ff04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b476cdb-42b4-47c3-b5ce-79bf3a6a8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentencepiece\n",
      "Version: 0.2.0\n",
      "Summary: SentencePiece python wrapper\n",
      "Home-page: https://github.com/google/sentencepiece\n",
      "Author: Taku Kudo\n",
      "Author-email: taku@google.com\n",
      "License: Apache\n",
      "Location: c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ce825-21bd-40aa-9126-792a66a05af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "print(\"SentencePiece imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30263080-b0e4-4c65-87b4-0ccdeb09851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "try:\n",
    "    tokenizer=T5Tokenizer.from_pretrained('t5-small')\n",
    "    print(\"T5Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c1ee38-b3ab-4f33-ae58-95f210f1f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearProjectionWithConv(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(LinearProjectionWithConv, self).__init__()\n",
    "        # Kernel size is (183, 1) and stride is (183, 1)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=embed_dim, kernel_size=(183, 1), stride=(183, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 183, 1024)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, 183, 1024)\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        x = x.squeeze(2)  # Remove the channel dimension: (batch_size, embed_dim, new_sequence_length)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc467f07-f12c-49e6-9385-c7ae382ae3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def  find_files(directory, pattern='**/*.json'):\n",
    "    return glob(os.path.join(directory,pattern),recursive=True)\n",
    "\n",
    "# Define paths to your folders\n",
    "train_folder = 'D:/Vathsala/train_2D_keypoints/openpose_output/output'\n",
    "val_folder = 'D:/Vathsala/val_2D_keypoints/openpose_output/output'\n",
    "test_folder = 'D:/Vathsala/test_2D_keypoints/openpose_output/output'\n",
    "\n",
    "# Load the datasets\n",
    "train_data = find_files(train_folder)\n",
    "val_data = find_files(val_folder)\n",
    "test_data = find_files(test_folder)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and Dataset Preparation\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "max_len = 512\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "\n",
    "train_dataset = TextDataset(train_data, tokenizer, max_len)\n",
    "val_dataset = TextDataset(val_data, tokenizer, max_len)\n",
    "test_dataset = TextDataset(test_data, tokenizer, max_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0746209-ec88-4e00-a37f-75504921bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class T5WithProjection(nn.Module):\n",
    "    def __init__(self, t5_model_name, embed_dim):\n",
    "        super(T5WithProjection, self).__init__()\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "        self.projection = LinearProjectionWithConv(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "         # Apply linear projection\n",
    "        input_ids = self.projection(outputs.logits)  # Apply projection to logits\n",
    "        # Forward pass through T5 model\n",
    "        outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "       \n",
    "        return outputs\n",
    "\n",
    "# Initialize the model with projection\n",
    "embed_dim = 1  # Adjust according to your T5 model's hidden size\n",
    "model = T5WithProjection('t5-small', embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ee4eaf-01ac-487b-9bab-0c33b3023ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cea9ed43c944ca8914fc398a968ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8c86e343504697b4caad9288343ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f42c42a688420ab822dfe2748166c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  1566,    12,  2968,    10,    37,   629,    19,  1627,     5,\n",
      "             1]])\n",
      "tensor([[  644,  4598,   229, 19250,     5,     1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the optimizer\\noptimizer = AdamW(model.parameters(), lr=5e-5)\\n\\n# Define your dataset and dataloader\\ntrain_dataset =  TextDataset(tokenizer,max_len,max_len)  \\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\\n\\n# Define the training function\\ndef train_epoch(model, dataloader, optimizer, device):\\n    model.train()\\n    total_loss = 0\\n    for batch in dataloader:\\n        input_ids = batch[\\'input_ids\\'].to(device)\\n        attention_mask = batch[\\'attention_mask\\'].to(device)\\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\\n        logits = outputs.logits\\n        logits = logits.view(-1, logits.size(-1))\\n        labels = input_ids.view(-1)\\n        loss = nn.functional.cross_entropy(logits, labels)\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n    return total_loss / len(dataloader)\\n\\n# Train the model\\nfor epoch in range(3):  # Number of epochs\\n    train_loss = train_epoch(model, train_dataloader, optimizer, device)\\n    print(f\"Epoch {epoch + 1}: Training loss = {train_loss:.4f}\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import T5ForConditionalGeneration, AdamW, T5Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# test\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"example sentence\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(input_ids)\n",
    "print(labels)\n",
    "\"\"\"\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define your dataset and dataloader\n",
    "train_dataset =  TextDataset(tokenizer,max_len,max_len)  \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the training function\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = input_ids.view(-1)\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}: Training loss = {train_loss:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ee6038-5c60-4f12-aca2-d853cbc600d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-with-projection\\\\tokenizer_config.json',\n",
       " 't5-with-projection\\\\special_tokens_map.json',\n",
       " 't5-with-projection\\\\spiece.model',\n",
       " 't5-with-projection\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('t5-with-projection')\n",
    "tokenizer.save_pretrained('t5-with-projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65c28b-8028-4960-9c72-be9bfba6dc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48faac-b0f6-4b7b-b5db-1b0562036c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
